# Copyright 2025 The game_arena Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Model generation implementations using HTTP POST APIs."""

import asyncio
import base64
import dataclasses
import datetime
import json
import multiprocessing
import os
import re
import threading
import time
from typing import Any, Mapping, Sequence

import aiohttp
import requests
from absl import logging

from game_arena.harness import model_generation, tournament_util

DEEPSEEK_THOUGHT_TAG_START = "<think>"


def _sanitize_url_for_logging(url: str) -> str:
    """Sanitize URL by removing potential API keys or passwords for safe logging.

    Args:
        url: The URL to sanitize

    Returns:
        Sanitized URL with sensitive parameters redacted
    """
    # Remove common API key parameters
    sanitized = re.sub(r'([?&])(api_key|key|token|password|secret)=[^&]*',
                       r'\1\2=***', url, flags=re.IGNORECASE)

    # Remove API keys in path segments (e.g., /api/v1/API_KEY_HERE/...)
    sanitized = re.sub(r'/[A-Za-z0-9_-]{20,}/', '/***/', sanitized)

    return sanitized
DEEPSEEK_THOUGHT_TAG_END = "</think>"


def _deepseek_separate_main_response_and_thoughts(
    response: str,
) -> tuple[str, str] | None:
    """Separates the main response and thoughts from a Deepseek response.

    Args:
      response: The response from the Deepseek model.

    Returns:
      A tuple of the main response and thoughts, or None if the tags are not
      present.
    """
    if DEEPSEEK_THOUGHT_TAG_START not in response:
        return None
    else:
        thoughts_and_response = response.split(DEEPSEEK_THOUGHT_TAG_START)[1]
        if DEEPSEEK_THOUGHT_TAG_END in thoughts_and_response:
            thoughts = thoughts_and_response.split(DEEPSEEK_THOUGHT_TAG_END)[0]
            main_response = thoughts_and_response.split(DEEPSEEK_THOUGHT_TAG_END)[1]
            return main_response, thoughts


def _create_image_text_content(
    model_input: tournament_util.ModelImageTextInput,
):
    """Creates content payload with text followed by image."""
    content = [{"type": "text", "text": model_input.prompt_text}]
    base64_image = base64.b64encode(model_input.prompt_image_bytes).decode("utf-8")
    content.append(
        {
            "type": "image_url",
            "image_url": {
                "url": (
                    f"data:{model_input.prompt_image_mime_type};base64,{base64_image}"
                )
            },
        }
    )
    return content


class TogetherAIModel(model_generation.MultimodalModel):
    """Wrapper for access to models served by Together.AI."""

    def __init__(
        self,
        model_name: str,
        *,
        model_options: Mapping[str, Any] | None = None,
        api_options: Mapping[str, Any] | None = None,
        api_key: str | None = None,
    ):
        super().__init__(
            model_name, model_options=model_options, api_options=api_options
        )
        # If API key is None, defaults to TOGETHER_API_KEY in environment.
        if api_key is None:
            try:
                api_key = os.environ["TOGETHER_API_KEY"]
            except KeyError as e:
                logging.error(
                    "TOGETHER_API_KEY environment variable not set. Please set it to"
                    " use %s.",
                    self._model_name,
                )
                raise e
        self._headers = {"Authorization": f"Bearer {api_key}"}

    # TODO(google-deepmind): Add error handling.
    def _generate(
        self,
        content: Sequence[Mapping[str, Any]],
        system_instruction: str | None = None,
    ) -> tournament_util.GenerateReturn:
        messages = []
        if system_instruction is not None:
            messages.append({"role": "system", "content": system_instruction})
        messages.append(
            {
                "role": "user",
                "content": content,
            }
        )

        if self._model_options is None:
            self._model_options = {}
        if self._api_options is None:
            self._api_options = {}

        request_options = {}

        if "max_tokens" in self._model_options:
            max_sampling_length = self._model_options["max_tokens"]
        else:
            match self._model_name:
                case "deepseek-ai/DeepSeek-R1-0528-tput":
                    max_context_length = 163839
                case "deepseek-ai/DeepSeek-R1":
                    max_context_length = 163839
                case "deepseek-ai/DeepSeek-V3":
                    max_context_length = 163839
                case "moonshotai/Kimi-K2-Instruct":
                    max_context_length = 128000
                case "Qwen/Qwen3-235B-A22B-Thinking-2507":
                    max_context_length = 262144
                case "Qwen/Qwen3-235B-A22B-Instruct-2507-tput":
                    max_context_length = 262144
                case "Qwen/Qwen3-235B-A22B-fp8-tput":
                    max_context_length = 40960
                case _:
                    logging.warning(
                        "max_context_length unknown for model %s. Defaulting to 32768.",
                        self._model_name,
                    )
                    max_context_length = 32768
            max_sampling_length = max_context_length - self._model_options.get(
                "max_prompt_length", 16384
            )

        request_options["max_tokens"] = max_sampling_length

        for supported_model_option in [
            "temperature",
            "top_k",
            "top_p",
        ]:
            if supported_model_option in self._model_options:
                request_options[supported_model_option] = self._model_options[
                    supported_model_option
                ]
        request = {
            "model": self._model_name,
            "messages": messages,
        } | request_options

        parallel_attempts = self._api_options.get("parallel_attempts", 1)
        if parallel_attempts > 1:
            payload_details = {
                "url": "https://api.together.xyz/v1/chat/completions",
                "payload": request,
                "headers": self._headers,
                "timeout": datetime.timedelta(
                    seconds=self._api_options.get("timeout", 3600)
                ),
            }
            name_to_payload_details = {
                f"{self._model_name}_attempt_{i}": payload_details
                for i in range(parallel_attempts)
            }
            maybe_successful_result, error_infos = asyncio.run(
                _post_requests_async_return_first_success(name_to_payload_details)
            )
            if maybe_successful_result is None:
                raise RuntimeError(
                    f"All {parallel_attempts} attempts failed: {error_infos}"
                )
            completion = maybe_successful_result.json_response
        else:
            # Splitting the request into a thread to be able to inspect the status of
            # the request and print the progress.
            q = multiprocessing.Queue()

            def do_request():
                try:
                    assert self._api_options is not None
                    response = requests.post(
                        "https://api.together.xyz/v1/chat/completions",
                        json=request,
                        headers=self._headers,
                        timeout=self._api_options.get("timeout", 3600),
                    )
                    response.raise_for_status()
                    q.put(
                        {"status_code": response.status_code, "json": response.json()}
                    )
                except requests.exceptions.RequestException as e:
                    logging.exception("Request to Together.AI failed: %s", e)
                    q.put(e)

            thread = threading.Thread(target=do_request)
            thread.start()

            last_wait_message_time = time.monotonic()
            while thread.is_alive():
                thread.join(timeout=1.0)
                current_time = time.monotonic()
                if current_time - last_wait_message_time >= 120:  # 2 minutes
                    print(f"Still waiting on {self._model_name}")
                    last_wait_message_time = current_time

            result = q.get()
            if isinstance(result, Exception):
                raise result

            assert result["status_code"] == 200

            completion = result["json"]

        content = completion["choices"][0]["message"]["content"]
        if content is None:
            logging.warning(
                "Together.AI Completion return content is None. Returning empty"
                " string. Request: %s",
                request,
            )
            content = ""

        main_response = content
        main_response_and_thoughts = ""

        if "deepseek" in self._model_name.lower():
            maybe_separated = _deepseek_separate_main_response_and_thoughts(content)
            if maybe_separated is not None:
                main_response = maybe_separated[0]
                main_response_and_thoughts = content

        generation_tokens = None
        prompt_tokens = None
        if "usage" in completion:
            usage = completion["usage"]
            generation_tokens = usage["completion_tokens"]
            prompt_tokens = usage["prompt_tokens"]

        return tournament_util.GenerateReturn(
            main_response=main_response,
            main_response_and_thoughts=main_response_and_thoughts,
            request_for_logging=request,
            response_for_logging=completion,
            generation_tokens=generation_tokens,
            prompt_tokens=prompt_tokens,
        )

    def generate_with_text_input(
        self, model_input: tournament_util.ModelTextInput
    ) -> tournament_util.GenerateReturn:
        content = [{"type": "text", "text": model_input.prompt_text}]
        return self._generate(content, model_input.system_instruction)

    def generate_with_image_text_input(
        self, model_input: tournament_util.ModelImageTextInput
    ) -> tournament_util.GenerateReturn:
        content = _create_image_text_content(model_input)
        return self._generate(content, model_input.system_instruction)


class XAIModel(model_generation.MultimodalModel):
    """Wrapper to call the xAI API. Supports both streaming and non-streaming."""

    def __init__(
        self,
        model_name: str,
        *,
        model_options: Mapping[str, Any] | None = None,
        api_options: Mapping[str, Any] | None = None,
        api_key: str | None = None,
        debug: bool = False,
    ):
        super().__init__(
            model_name, model_options=model_options, api_options=api_options
        )
        if self._model_options is None:
            self._model_options = {}
        if self._api_options is None:
            self._api_options = {}

        if api_key is None:
            api_key = os.environ["XAI_API_KEY"]

        if api_key is None:
            raise ValueError("XAI API key not found.")

        self._headers = {"Authorization": f"Bearer {api_key}"}
        self._debug = debug
        self._stream = self._api_options.get("stream", True)

    # TODO(google-deepmind): Add error handling.
    def _post_request(self, request: Mapping[str, Any], stream: bool):
        """Sends a POST request to the xAI API and handles errors."""
        timeout = self._api_options.get("timeout", 20 * 60)
        try:
            response = requests.post(
                "https://api.x.ai/v1/chat/completions",
                json=request,
                headers=self._headers,
                stream=stream,
                timeout=timeout,
            )
            response.raise_for_status()

            return response
        except requests.exceptions.HTTPError as e:
            if e.response is not None:
                print("Error during XAI request: ", flush=True)
                print(f"Status code: {e.response.status_code}", flush=True)
                print(f"Reason: {e.response.reason}", flush=True)
                print(f"Headers: {e.response.headers}", flush=True)
                print(f"Response text: {e.response.text}", flush=True)
                print(request)
                if e.response.status_code == 400:
                    raise model_generation.DoNotRetryError(
                        str(e),
                        info={"request": request, "response": e.response},
                    ) from e
            raise

    def _generate(
        self,
        messages: Sequence[Mapping[str, Any]],
    ) -> tournament_util.GenerateReturn:
        request = {
            "model": self._model_name,
            "messages": messages,
            "stream": False,
            "stream_options": {"include_usage": True},
        }
        for option in ["temperature", "top_p", "top_k"]:
            if option in self._model_options:
                request[option] = self._model_options[option]

        response = self._post_request(request, stream=False)
        completion = response.json()
        full_content = completion["choices"][0]["message"]["content"]

        if full_content is None:
            logging.warning(
                "xAI Completion return content is None. Returning empty string."
                " Request: %s",
                request,
            )
            full_content = ""

        full_reasoning_content = ""
        if "reasoning_content" in completion["choices"][0]["message"]:
            full_reasoning_content = completion["choices"][0]["message"][
                "reasoning_content"
            ]
        total_generation_tokens = None
        total_prompt_tokens = None
        total_reasoning_tokens = None
        if "usage" in completion:
            usage = completion["usage"]
            if "completion_tokens" in usage:
                total_generation_tokens = usage["completion_tokens"]
            if "prompt_tokens" in usage:
                total_prompt_tokens = usage["prompt_tokens"]
            if (
                "completion_tokens_details" in usage
                and "reasoning_tokens" in usage["completion_tokens_details"]
            ):
                total_reasoning_tokens = usage["completion_tokens_details"][
                    "reasoning_tokens"
                ]
        main_response_and_thoughts = (
            full_content + ("\n\n" + full_reasoning_content)
            if full_reasoning_content
            else ""
        )
        return tournament_util.GenerateReturn(
            main_response=full_content,
            main_response_and_thoughts=main_response_and_thoughts,
            request_for_logging=request,
            response_for_logging=completion,
            generation_tokens=total_generation_tokens,
            prompt_tokens=total_prompt_tokens,
            reasoning_tokens=total_reasoning_tokens,
        )

    def _print_debug(self, message: str):
        if self._debug:
            print(message)

    def _log_chunk_received(
        self,
        start_time: float,
        elapsed_time: float,
        full_content: str,
        delta_content: str,
        is_thinking_chunk: bool,
        last_full_content_print_time: float,
        current_time: float,
    ):
        """Logs information about received chunks."""
        time_since_last_full_print = current_time - last_full_content_print_time
        if (
            last_full_content_print_time == start_time
            or time_since_last_full_print >= 120
        ):
            if is_thinking_chunk:
                self._print_debug(
                    f"[{self._model_name}] Received thinking chunk, elapsed:"
                    f" {elapsed_time:.2f}s, content_len:"
                    f" {len(full_content)}, full_content: {full_content}"
                )
            else:
                self._print_debug(
                    f"[{self._model_name}] Received chunk, elapsed:"
                    f" {elapsed_time:.2f}s, content_len:"
                    f" {len(full_content)}, full_content: {full_content}"
                )
            return True
        else:
            if is_thinking_chunk:
                self._print_debug(
                    f"[{self._model_name}] Received thinking chunk, elapsed:"
                    f" {elapsed_time:.2f}s, chunk: {delta_content}"
                )
            else:
                self._print_debug(
                    f"[{self._model_name}] Received chunk, elapsed:"
                    f" {elapsed_time:.2f}s, chunk: {delta_content}"
                )
            return False

    def _generate_streaming(
        self,
        messages: Sequence[Mapping[str, Any]],
    ) -> tournament_util.GenerateReturn:
        request = {
            "model": self._model_name,
            "messages": messages,
            "stream": True,
            "stream_options": {"include_usage": True},
        }

        full_content = ""
        full_reasoning_content = ""
        total_generation_tokens = 0
        total_prompt_tokens = 0
        total_reasoning_tokens = 0
        response_for_logging = []
        start_time = time.monotonic()
        last_full_content_print_time = start_time
        self._print_debug(f"[{self._model_name}] Starting streaming completion.")

        response = self._post_request(request, stream=True)
        try:
            for chunk in response.iter_lines():
                if chunk:
                    current_time = time.monotonic()
                    elapsed_time = current_time - start_time
                    if chunk.startswith(b"data: "):
                        chunk_data = chunk[len(b"data: ") :].decode("utf-8")
                        if chunk_data == "[DONE]":
                            self._print_debug(
                                f"[{self._model_name}] Streaming finished, elapsed:"
                                f" {elapsed_time:.2f}s"
                            )
                            break
                        try:
                            json_data = json.loads(chunk_data)
                            if "choices" in json_data and json_data["choices"]:
                                delta = json_data["choices"][0].get("delta", {})
                                if "content" in delta:
                                    delta_content = delta["content"]
                                    full_content += delta_content
                                    is_full_content_print = self._log_chunk_received(
                                        start_time=start_time,
                                        elapsed_time=elapsed_time,
                                        full_content=full_content,
                                        delta_content=delta_content,
                                        is_thinking_chunk=False,
                                        last_full_content_print_time=last_full_content_print_time,
                                        current_time=current_time,
                                    )
                                    if is_full_content_print:
                                        # Log a chunk only every 2 minutes.
                                        response_for_logging.append(
                                            {
                                                "chunk": chunk_data,
                                                "elapsed_time": elapsed_time,
                                                "full_content": full_content,
                                            }
                                        )
                                        last_full_content_print_time = current_time
                                if "reasoning_content" in delta:
                                    delta_reasoning_content = delta["reasoning_content"]
                                    full_reasoning_content += delta_reasoning_content
                                    is_full_content_print = self._log_chunk_received(
                                        start_time=start_time,
                                        elapsed_time=elapsed_time,
                                        full_content=full_reasoning_content,
                                        delta_content=delta_reasoning_content,
                                        is_thinking_chunk=True,
                                        last_full_content_print_time=last_full_content_print_time,
                                        current_time=current_time,
                                    )
                                    if is_full_content_print:
                                        response_for_logging.append(
                                            {
                                                "chunk": chunk_data,
                                                "elapsed_time": elapsed_time,
                                                "full_content": full_reasoning_content,
                                            }
                                        )
                                        last_full_content_print_time = current_time

                            if "usage" in json_data:
                                usage = json_data["usage"]
                                response_for_logging.append({"usage_chunk": json_data})
                                if "completion_tokens" in usage:
                                    total_generation_tokens += usage[
                                        "completion_tokens"
                                    ]
                                if "prompt_tokens" in usage:
                                    total_prompt_tokens += usage["prompt_tokens"]
                                if (
                                    "completion_tokens_details" in usage
                                    and "reasoning_tokens"
                                    in usage["completion_tokens_details"]
                                ):
                                    total_reasoning_tokens += usage[
                                        "completion_tokens_details"
                                    ]["reasoning_tokens"]

                        except json.JSONDecodeError as e:
                            logging.warning(
                                "[%s] JSON decode error for chunk: %s, elapsed: %.2fs",
                                self._model_name,
                                chunk_data,
                                elapsed_time,
                            )
                            raise e
        finally:
            response.close()

        main_response_and_thoughts = (
            full_content + ("\n\n" + full_reasoning_content)
            if full_reasoning_content
            else ""
        )
        return tournament_util.GenerateReturn(
            main_response=full_content,
            main_response_and_thoughts=main_response_and_thoughts,
            request_for_logging=request,
            response_for_logging={"filtered_chunks_list": response_for_logging},
            generation_tokens=total_generation_tokens,
            prompt_tokens=total_prompt_tokens,
            reasoning_tokens=total_reasoning_tokens,
        )

    def generate_with_text_input(
        self, model_input: tournament_util.ModelTextInput
    ) -> tournament_util.GenerateReturn:
        messages = []
        if model_input.system_instruction is not None:
            messages.append(
                {"role": "system", "content": model_input.system_instruction}
            )

        messages.append(
            {
                "role": "user",
                "content": model_input.prompt_text,
            }
        )

        if self._stream:
            return self._generate_streaming(messages)
        else:
            return self._generate(messages)

    def generate_with_image_text_input(
        self, model_input: tournament_util.ModelImageTextInput
    ) -> tournament_util.GenerateReturn:
        if "grok-3" in self._model_name:
            raise model_generation.UnsupportedCapabilityError(
                f"Model {self._model_name} does not support image input."
            )
        messages = []
        if model_input.system_instruction is not None:
            messages.append(
                {"role": "system", "content": model_input.system_instruction}
            )
        content = _create_image_text_content(model_input)
        messages.append(
            {
                "role": "user",
                "content": content,
            }
        )
        if self._stream:
            return self._generate_streaming(messages)
        else:
            return self._generate(messages)


@dataclasses.dataclass(frozen=True, kw_only=True)
class PostRequestResult:
    name: str
    status: int
    json_response: Mapping[str, Any]


async def _post_request_async(
    *,
    session: aiohttp.ClientSession,
    name: str,
    url: str,
    payload: Mapping[str, Any],
    headers: Mapping[str, str] | None = None,
    timeout: datetime.timedelta,
) -> PostRequestResult:
    """Posts a request asynchronously and returns the result."""
    sanitized_url = _sanitize_url_for_logging(url)
    logging.debug("Starting POST for '%s' to %s", name, sanitized_url)
    try:
        async with session.post(
            url, json=payload, headers=headers, timeout=timeout.total_seconds()
        ) as response:
            logging.info("Finished POST for '%s' with status: %d", name, response.status)
            # Raise an exception for bad status codes (4xx or 5xx)
            response.raise_for_status()

            json_response = await response.json()
            return PostRequestResult(
                name=name, status=response.status, json_response=json_response
            )

    except asyncio.CancelledError:
        logging.warning("POST for '%s' was cancelled.", name)
        raise
    except Exception as e:
        logging.error("POST for '%s' failed: %s", name, type(e).__name__)
        # Re-raise the exception so the main loop knows it failed
        raise


async def _post_requests_async_return_first_success(
    name_to_payload_details: Mapping[str, Mapping[str, Any]],
) -> tuple[PostRequestResult | None, Sequence[Mapping[str, Any]]]:
    """Posts requests asynchronously and returns the first successful result."""
    error_infos = []
    successful_result = None

    async def _wrapper(
        *,
        session: aiohttp.ClientSession,
        name: str,
        url: str,
        payload: Mapping[str, Any],
        headers: Mapping[str, str] | None = None,
        timeout: datetime.timedelta,
    ) -> tuple[str, PostRequestResult | None, Exception | None]:
        """Wraps the async post request to return name and exception."""
        try:
            result = await _post_request_async(
                session=session,
                name=name,
                url=url,
                payload=payload,
                headers=headers,
                timeout=timeout,
            )
            return name, result, None
        except Exception as e:  # pylint: disable=broad-exception-caught
            return name, None, e

    async with aiohttp.ClientSession() as session:
        if not name_to_payload_details:
            return None, []

        tasks = [
            asyncio.create_task(_wrapper(session=session, name=name, **payload_details))
            for name, payload_details in name_to_payload_details.items()
        ]

        for future in asyncio.as_completed(tasks):
            name, result, exc = await future
            if exc is None:
                successful_result = result
                print(f"\n--- First SUCCESSFUL POST received from '{name}' ---")
                break  # We have a winner
            else:
                print(f"Ignoring failed task '{name}'. Waiting for the next one...")
                error_infos.append(
                    {
                        "request_name": name,
                        "error": type(exc).__name__,
                        "message": str(exc),
                    }
                )
        else:
            print("\n--- All POST requests failed. No successful result found. ---\n")

        # Cancel remaining tasks
        for task in tasks:
            if not task.done():
                task.cancel()
        await asyncio.gather(*tasks, return_exceptions=True)

        return successful_result, error_infos
