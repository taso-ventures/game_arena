# LLM API Keys - Copy to .env and fill in your actual keys
GEMINI_API_KEY=your_gemini_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Additional LLM Provider API Keys
XAI_API_KEY=your_xai_api_key_here
TOGETHER_API_KEY=your_together_api_key_here
HF_TOKEN=your_huggingface_token_here

# Local Model Inference
# Ollama base URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# FreeCiv3D Server Connection (adjust if running on different host/port)
FREECIV_SERVER_URL=http://host.docker.internal:8080
FREECIV_WS_URL=ws://host.docker.internal:4002

# FreeCiv3D LLM Gateway Configuration
FREECIV_GATEWAY_HOST=fciv-net
FREECIV_GATEWAY_PORT=8003
FREECIV_GATEWAY_WS_PATH=/ws/agent/{agent_id}

# Rate Limit Compliance
# Note: Gateway burst limit is 40 msg/s, but 24 msg/turn is recommended for 2-player games
FREECIV_MAX_MESSAGES_PER_TURN=24
# Note: Using 10.0s cache TTL (not FreeCiv3D's recommended 1.5s) to reduce query frequency
FREECIV_STATE_CACHE_TTL=10.0
FREECIV_ADAPTIVE_BACKOFF=true

# Session Management
FREECIV_SESSION_RESUMPTION=true
FREECIV_RECONNECT_WINDOW=60

# Game Configuration
GAME_TYPE=freeciv

# Demo/Testing Configuration
NUM_MOVES=10
GEMINI_MODEL=gemini-2.5-flash
OPENAI_MODEL=gpt-4.1
PARSER_CHOICE=rule_then_soft

# Model-specific configurations
# Ollama models (for local inference)
OLLAMA_MODEL=llama3.2:3b

# HuggingFace models
HF_MODEL=meta-llama/Llama-3.2-3B-Instruct

# xAI models
XAI_MODEL=grok-4

# Together.AI models (Kimi K2, DeepSeek, Qwen, etc.)
TOGETHER_MODEL=moonshotai/Kimi-K2-Instruct

# FreeCiv-specific settings
FREECIV_MAP_SIZE=small
FREECIV_TURN_LIMIT=200
FREECIV_GAME_SPEED=fast
